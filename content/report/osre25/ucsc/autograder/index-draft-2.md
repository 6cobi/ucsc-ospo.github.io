---
title: "LINQS: Autograder (LLM Detection)"
subtitle: A Tool for Detecting AI-Generated Code in Academic Submissions
summary: This project aims to help academic institutions in identify AI-generated code in student assignments, supporting fair grading and upholding academic integrity.
authors: 
  - anvichip
tags: ["osre25"]
categories: ["Artificial Intelligence","Machine Learning", "LLMs"]
date: 2025-06-14
lastmod: 2025-06-14
featured: false
draft: true

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: "Center"
  preview_only: false
---

# LINQS: Autograder (GSoC '25)

As part of the [LINQS: Autograder (LLM Detection)](/content/project/osre25/ucsc/autograder/index.md) my [proposal](https://summerofcode.withgoogle.com/programs/2025/projects/jxBUpvoM) under the mentorship of Eriq Augistine, Lucas Ellenberger, and Lise Getoor aims to build a tool for AI plag detection in code.

## Problem Statement

With the rise of Large Language Models and tools like ChatGPT and GitHub Copilot, and their easier accessibility to students, academic institutions are facing new sets of **challenges in maintaining academic integrity**. Students are increasingly using these tools for assistance with their courseworks especially in programming assignments. 

While these tools can are useful for purposes such as brainstorming, research, and drafting, its use in completing assignments often crosses ethical boundaries. The usage of these tools by students make it hard to **uphold fairness while grading students and ensure they are truly learning.** 

AI-generated code often lacks unique identifiers, rendering traditional plagiarism detectors like MOSS ineffective in detecting AI-generated code. That’s why there is a **need for better systems** that can assess whether that code was AI generated by spotting underlying patterns.


## Project Overview:

This is the problem that I am working to address with my project **‘LLM Detection’**. 

I aim to build a system that helps academic institutions ensure fairness and integrity in students' work. 
To accomplish this goal, I will be working on 2 tasks: 
* Building a tool which determines whether a given piece of code was written by AI or not.  
* Designing and implementing a mechanism to compute a confidence score that indicates the likelihood of AI involvement in the code.

This tool can discourage them from copying or completing entire assignments using AI tools, encouraging honest and independent work.

(Read my full GSoC proposal here: [Proposal](https://drive.google.com/file/d/1skTVhcrEMAAwc6XzYQ0w3_uVRLxz0IB9/view?usp=sharing))

## About me:

Hey there!

My name is Anvi Kohli, I am a senior majoring in Computer Science and AI from India. This summer I will be contributing to the Autograder project by the LINQS Lab, under the guidance of Eriq Augistine, Lucas Ellenberger, and Lise Getoor. 

A problem-solver at heart, I love to brainstorm, solve, and optimize complex issues. An instance being reaching the grand finals of the Smart India Hackathon to become the third best team nationwide with our app – “PM Poshan”. This app was built to digitize the monitoring and functioning of the mid-day meal scheme in India. It gave me the opportunity to improve my versatility and exposed me to all stages of the product development cycle.

I have hands-on experience in a multitude of domains such as AI/Data Science, cloud, full-stack development, and DevOps. Within AI, I have worked in GenAI, Computer Vision, Deep Learning and Classical Machine Learning. Apart from this I have a strong interest in entrepreneurship, travelling, and cooking.







